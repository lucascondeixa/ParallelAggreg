{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics, OrderedCollections, Dates, Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"aggreg.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using EnergySystemModeling: SeriesInstance,\n",
    "#     ClustInstance,\n",
    "#     AggregInstance,\n",
    "#     DistUpdate,\n",
    "#     load_series_instance,\n",
    "#     load_clust_instance,\n",
    "#     aggreg1D,\n",
    "#     cdad,\n",
    "#     search_min_dist,\n",
    "#     compute_dist,\n",
    "#     update_marker,\n",
    "#     replace_lines,\n",
    "#     update_clust!,\n",
    "#     update_k!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new copy method to copy _ClustInstance and _SeriesInstance\n",
    "Base.copy(x::T) where T = T([getfield(x, k) for k âˆˆ fieldnames(T)]...);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_paral (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function test_paral(series::VecOrMat{Float64},\n",
    "    block_size::Int,\n",
    "    stopping_k::Int,\n",
    "    current_k::Int,\n",
    "    dm::Symbol,\n",
    "    rep_value::Symbol,\n",
    "    lseries::Int,\n",
    "    nseries::Int,\n",
    "    series_dc::VecOrMat{Float64},\n",
    "    ord_dc::VecOrMat{Int},\n",
    "    k_cent::VecOrMat{Float64},\n",
    "    weights::Vector{Int},\n",
    "    series_clust::Vector{Int},\n",
    "    nclusters::Int,\n",
    "    search_range::UnitRange,\n",
    "    dc_mode::Bool,\n",
    "    _SeriesInstance::SeriesInstance,\n",
    "    _ClustInstance::ClustInstance,\n",
    "    _DistUpdate::Dict{Vector{Bool},DistUpdate},\n",
    "    _SeriesUpdate::Dict{String,SeriesInstance},\n",
    "    _ClustUpdate::Dict{String,ClustInstance})\n",
    "\n",
    "    # @info \"Clustering the next step\"\n",
    "    k = lseries\n",
    "    @time while k >= stopping_k + block_size - 1\n",
    "        \n",
    "        # Vector with distances (to be updated as it goes)\n",
    "        dist = Vector{Float64}(undef,length(search_range))\n",
    "\n",
    "        # Sets\n",
    "        N = 1:nseries\n",
    "        K = copy(search_range)\n",
    "\n",
    "        # Compute the distance for each aggregation (i.e., changing the merging_clust)\n",
    "        # TODO: implement parallelisation such as '@async Threads.@threads @inbounds for k in K'\n",
    "        @inbounds for k_search in K\n",
    "            # Merging to be tested (neighbouring hypothesis)\n",
    "            # TODO: implement non-neighbouring hypothesis\n",
    "            merging_clust = k_search:k_search+block_size-1\n",
    "            # Create a temporary marker to merge the clusters tested\n",
    "            marker_temp = [sc in merging_clust for sc in series_clust]\n",
    "\n",
    "            # Separation needed for duration curves analysis\n",
    "            if dc_mode\n",
    "                # Create a temp marker for the elements in between clustered [min,max] order\n",
    "                marker_temp_dc = minimum(ord_dc[marker_temp,:]):maximum(ord_dc[marker_temp,:])\n",
    "                # Using duration curves chunks (in between the min and max values of marker_temp)\n",
    "                series_comp = sort(series, dims=1,rev=true)[marker_temp_dc,:]\n",
    "                # Forming the centroids\n",
    "                k_cent_comp = copy(series)\n",
    "                (k_cent_comp[marker_temp,:],) = aggreg1D(series[marker_temp,:], rep_value)\n",
    "                # Ordering chunk with clustering instance in a decrescent order\n",
    "                k_cent_comp = sort(k_cent_comp, dims=1, rev=true)[marker_temp_dc,:]\n",
    "            else\n",
    "                # Part of series compared\n",
    "                series_comp = series[marker_temp,:]\n",
    "                # Centroids of the temporarily formed cluster (TODO: implement another method for aggreg1D receiving the clusters with respective weights)\n",
    "                (k_cent_comp,) = aggreg1D(series_comp, rep_value)\n",
    "            end\n",
    "            \n",
    "            # Distance computation\n",
    "            dist[k_search] = compute_dist(N, dm, series_comp, k_cent_comp)\n",
    "        end\n",
    "\n",
    "        # Find whenever the min_dist occurs first (i.e., using findmin()[2])\n",
    "        ## TODO: implement the multiple merges (e.g., using findall())\n",
    "        min_dist = findmin(dist)[2] |> Int        \n",
    "        merging_clust = min_dist:min_dist+block_size-1\n",
    "\n",
    "        # Create a flag to the positions in the series that will be aggregated\n",
    "        marker = update_marker(_SeriesInstance, _ClustInstance, min_dist)    \n",
    "\n",
    "        # Update _DistUpdate dictionary with the minimal distance found and the new marker\n",
    "        _DistUpdate = merge(+, _DistUpdate, Dict(marker => DistUpdate(min_dist,merging_clust)))\n",
    "\n",
    "        # Update clusters and series_clust\n",
    "        update_clust!(_ClustInstance, _SeriesInstance, min_dist)\n",
    "\n",
    "        # Update clustering values\n",
    "        series_clust = _ClustInstance.series_clust\n",
    "        weights = _ClustInstance.weights\n",
    "\n",
    "        nclusters = _ClustInstance.nclusters\n",
    "        k_cent = _ClustInstance.k_cent\n",
    "        search_range =_ClustInstance.search_range\n",
    "        \n",
    "        # Update number of clusters k\n",
    "        new_current_k = _ClustInstance.nclusters\n",
    "        update_k!(_SeriesInstance, new_current_k)\n",
    "        k = new_current_k\n",
    "\n",
    "        # Store series_clust and k_cent\n",
    "        _ClustUpdate = merge(+,_ClustUpdate,Dict(\"$k\" => copy(_ClustInstance)))\n",
    "        _SeriesUpdate = merge(+,_SeriesUpdate,Dict(\"$k\" => copy(_SeriesInstance)))\n",
    "\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "execute_inst (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function execute_inst(lseries::Int,nseries::Int,dc_mode::Bool)\n",
    "    \n",
    "    # Declare series\n",
    "    series = rand(lseries,nseries);\n",
    "    \n",
    "    # Forming SeriesInstance\n",
    "    block_size = 2\n",
    "    stopping_k = 1\n",
    "    current_k = lseries\n",
    "    rep_value = :mean\n",
    "    series_dc = sort(series, dims=1, rev=true)\n",
    "    ord_dc = reduce(hcat,sortperm.(collect(eachslice(series,dims=2)),rev=true))\n",
    "    \n",
    "    # Forming ClustInstance\n",
    "    k_cent = copy(series)\n",
    "    weights = ones(lseries) |> Vector{Int64}\n",
    "    series_clust = collect(1:lseries) |> Vector{Int64}\n",
    "    nclusters = lseries\n",
    "    search_range = 1:size(series,1)-block_size+1\n",
    "    dm = :ward\n",
    "\n",
    "    # Declaring instances\n",
    "    _SeriesInstance = load_series_instance(series,block_size,current_k,stopping_k,dm,rep_value,lseries,nseries,series_dc,ord_dc);\n",
    "    _ClustInstance = load_clust_instance(k_cent,series_clust,weights,search_range,dc_mode)\n",
    "\n",
    "    # Dictionary to keep the min distances and respective markers/min_dist found in each iteration\n",
    "    _DistUpdate = Dict{Vector{Bool}, DistUpdate}()\n",
    "\n",
    "    # Dictionaries to store series_clust and k_cent\n",
    "    _SeriesUpdate = Dict{String,SeriesInstance}()\n",
    "    _ClustUpdate = Dict{String,ClustInstance}();\n",
    "\n",
    "    \n",
    "    test_paral(series,block_size,\n",
    "    stopping_k,\n",
    "    current_k,\n",
    "    dm,\n",
    "    rep_value,\n",
    "    lseries,\n",
    "    nseries,\n",
    "    series_dc,\n",
    "    ord_dc,\n",
    "    k_cent,\n",
    "    weights,\n",
    "    series_clust,\n",
    "    nclusters,\n",
    "    search_range,\n",
    "    dc_mode,\n",
    "    _SeriesInstance,\n",
    "    _ClustInstance,\n",
    "    _DistUpdate,\n",
    "    _SeriesUpdate,\n",
    "    _ClustUpdate)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: 100 lines random series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.277453 seconds (329.49 k allocations: 153.613 MiB, 4.30% gc time)\n"
     ]
    }
   ],
   "source": [
    "execute_inst(100,4,true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.071736 seconds (210.69 k allocations: 25.153 MiB, 7.14% gc time)\n"
     ]
    }
   ],
   "source": [
    "execute_inst(100,4,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: 200 lines random series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.120620 seconds (1.30 M allocations: 1.089 GiB, 4.18% gc time)\n"
     ]
    }
   ],
   "source": [
    "execute_inst(200,4,true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.322298 seconds (821.50 k allocations: 122.399 MiB, 10.74% gc time)\n"
     ]
    }
   ],
   "source": [
    "execute_inst(200,4,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: 500 lines random series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 37.707016 seconds (8.17 M allocations: 16.596 GiB, 2.76% gc time)\n"
     ]
    }
   ],
   "source": [
    "execute_inst(500,4,true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2.415778 seconds (5.18 M allocations: 1.650 GiB, 6.71% gc time)\n"
     ]
    }
   ],
   "source": [
    "execute_inst(500,4,false)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": " 1.6.1",
   "language": "julia",
   "name": "-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
