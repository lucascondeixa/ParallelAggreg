{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Statistics, OrderedCollections, Dates, Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nthreads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"aggreg.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using EnergySystemModeling: SeriesInstance,\n",
    "#     ClustInstance,\n",
    "#     AggregInstance,\n",
    "#     DistUpdate,\n",
    "#     load_series_instance,\n",
    "#     load_clust_instance,\n",
    "#     aggreg1D,\n",
    "#     cdad,\n",
    "#     search_min_dist,\n",
    "#     compute_dist,\n",
    "#     update_marker,\n",
    "#     replace_lines,\n",
    "#     update_clust!,\n",
    "#     update_k!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new copy method to copy _ClustInstance and _SeriesInstance\n",
    "Base.copy(x::T) where T = T([getfield(x, k) for k âˆˆ fieldnames(T)]...);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_serial (generic function with 1 method)"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function test_serial(series::VecOrMat{Float64},\n",
    "    block_size::Int,\n",
    "    stopping_k::Int,\n",
    "    current_k::Int,\n",
    "    dm::Symbol,\n",
    "    rep_value::Symbol,\n",
    "    lseries::Int,\n",
    "    nseries::Int,\n",
    "    series_dc::VecOrMat{Float64},\n",
    "    ord_dc::VecOrMat{Int},\n",
    "    k_cent::VecOrMat{Float64},\n",
    "    weights::Vector{Int},\n",
    "    series_clust::Vector{Int},\n",
    "    nclusters::Int,\n",
    "    search_range::UnitRange,\n",
    "    dc_mode::Bool,\n",
    "    _SeriesInstance::SeriesInstance,\n",
    "    _ClustInstance::ClustInstance,\n",
    "    _DistUpdate::Dict{Vector{Bool},DistUpdate},\n",
    "    _SeriesUpdate::Dict{String,SeriesInstance},\n",
    "    _ClustUpdate::Dict{String,ClustInstance})\n",
    "\n",
    "    # @info \"Clustering the next step\"\n",
    "    k = lseries\n",
    "    @time while k >= stopping_k + block_size - 1\n",
    "        \n",
    "        # Vector with distances (to be updated as it goes)\n",
    "        dist = Vector{Float64}(undef,length(search_range))\n",
    "\n",
    "        # Sets\n",
    "        N = 1:nseries\n",
    "        K = copy(search_range)\n",
    "\n",
    "        # Compute the distance for each aggregation (i.e., changing the merging_clust)\n",
    "        # TODO: implement parallelisation such as '@async Threads.@threads @inbounds for k in K'\n",
    "        @inbounds for k_search in K\n",
    "            # Merging to be tested (neighbouring hypothesis)\n",
    "            # TODO: implement non-neighbouring hypothesis\n",
    "            merging_clust = k_search:k_search+block_size-1\n",
    "            # Create a temporary marker to merge the clusters tested\n",
    "            marker_temp = [sc in merging_clust for sc in series_clust]\n",
    "\n",
    "            # Separation needed for duration curves analysis\n",
    "            if dc_mode\n",
    "                # Create a temp marker for the elements in between clustered [min,max] order\n",
    "                if isempty(ord_dc[marker_temp,:])\n",
    "                    print(\"k_search = $k_search \\n\")\n",
    "                    println(\"ord_dc=\", ord_dc[marker_temp,:])\n",
    "                end\n",
    "                marker_temp_dc = minimum(ord_dc[marker_temp,:]):maximum(ord_dc[marker_temp,:])                \n",
    "                # Using duration curves chunks (in between the min and max values of marker_temp)\n",
    "                series_comp = sort(series, dims=1,rev=true)[marker_temp_dc,:]\n",
    "                # Forming the centroids\n",
    "                k_cent_comp = copy(series)\n",
    "                (k_cent_comp[marker_temp,:],) = aggreg1D(series[marker_temp,:], rep_value)\n",
    "                # Ordering chunk with clustering instance in a decrescent order\n",
    "                k_cent_comp = sort(k_cent_comp, dims=1, rev=true)[marker_temp_dc,:]\n",
    "            else\n",
    "                # Part of series compared\n",
    "                series_comp = series[marker_temp,:]\n",
    "                # Centroids of the temporarily formed cluster (TODO: implement another method for aggreg1D receiving the clusters with respective weights)\n",
    "                (k_cent_comp,) = aggreg1D(series_comp, rep_value)\n",
    "            end\n",
    "            \n",
    "            # Distance computation\n",
    "            dist[k_search] = compute_dist(N, dm, series_comp, k_cent_comp)\n",
    "        end\n",
    "\n",
    "        # Find whenever the min_dist occurs first (i.e., using findmin()[2])\n",
    "        ## TODO: implement the multiple merges (e.g., using findall())\n",
    "        min_dist = findmin(dist)[2] |> Int        \n",
    "        merging_clust = min_dist:min_dist+block_size-1\n",
    "\n",
    "        # Create a flag to the positions in the series that will be aggregated\n",
    "        marker = update_marker(_SeriesInstance, _ClustInstance, min_dist)    \n",
    "\n",
    "        # Update _DistUpdate dictionary with the minimal distance found and the new marker\n",
    "        _DistUpdate = merge(+, _DistUpdate, Dict(marker => DistUpdate(min_dist,merging_clust)))\n",
    "\n",
    "        # Update clusters and series_clust\n",
    "        update_clust!(_ClustInstance, _SeriesInstance, min_dist)\n",
    "\n",
    "        # Update clustering values\n",
    "        series_clust = _ClustInstance.series_clust\n",
    "        weights = _ClustInstance.weights\n",
    "\n",
    "        nclusters = _ClustInstance.nclusters\n",
    "        k_cent = _ClustInstance.k_cent\n",
    "        search_range =_ClustInstance.search_range\n",
    "        \n",
    "        # Update number of clusters k\n",
    "        new_current_k = _ClustInstance.nclusters\n",
    "        update_k!(_SeriesInstance, new_current_k)\n",
    "        k = new_current_k\n",
    "\n",
    "        # Store series_clust and k_cent\n",
    "        _ClustUpdate = merge(+,_ClustUpdate,Dict(\"$k\" => copy(_ClustInstance)))\n",
    "        _SeriesUpdate = merge(+,_SeriesUpdate,Dict(\"$k\" => copy(_SeriesInstance)))\n",
    "\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_parallel (generic function with 1 method)"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function test_parallel(series::VecOrMat{Float64},\n",
    "    block_size::Int,\n",
    "    stopping_k::Int,\n",
    "    current_k::Int,\n",
    "    dm::Symbol,\n",
    "    rep_value::Symbol,\n",
    "    lseries::Int,\n",
    "    nseries::Int,\n",
    "    series_dc::VecOrMat{Float64},\n",
    "    ord_dc::VecOrMat{Int},\n",
    "    k_cent::VecOrMat{Float64},\n",
    "    weights::Vector{Int},\n",
    "    series_clust::Vector{Int},\n",
    "    nclusters::Int,\n",
    "    search_range::UnitRange,\n",
    "    dc_mode::Bool,\n",
    "    _SeriesInstance::SeriesInstance,\n",
    "    _ClustInstance::ClustInstance,\n",
    "    _DistUpdate::Dict{Vector{Bool},DistUpdate},\n",
    "    _SeriesUpdate::Dict{String,SeriesInstance},\n",
    "    _ClustUpdate::Dict{String,ClustInstance})\n",
    "\n",
    "    # @info \"Clustering the next step\"\n",
    "    k = lseries\n",
    "    @time while k >= stopping_k + block_size - 1\n",
    "        \n",
    "        # Vector with distances (to be updated as it goes)\n",
    "        dist = Vector{Float64}(undef,length(search_range))\n",
    "\n",
    "        # Sets\n",
    "        N = 1:nseries\n",
    "        K = copy(search_range)\n",
    "\n",
    "        # Compute the distance for each aggregation (i.e., changing the merging_clust)\n",
    "        # TODO: implement parallelisation such as '@async Threads.@threads @inbounds for k in K'\n",
    "        Threads.@threads for k_search in K\n",
    "            # Merging to be tested (neighbouring hypothesis)\n",
    "            # TODO: implement non-neighbouring hypothesis\n",
    "            # NOTE: in the parallel version this assignment causes sync issues. Avoid symbolic assignments\n",
    "            # that share memory.\n",
    "#             merging_clust = k_search:k_search+block_size-1\n",
    "            # Create a temporary marker to merge the clusters tested\n",
    "            marker_temp = [sc in k_search:k_search+block_size-1 for sc in series_clust]\n",
    "\n",
    "            # Separation needed for duration curves analysis\n",
    "            if dc_mode\n",
    "                # Create a temp marker for the elements in between clustered [min,max] order\n",
    "                marker_temp_dc = minimum(ord_dc[marker_temp,:]):maximum(ord_dc[marker_temp,:])\n",
    "                # Using duration curves chunks (in between the min and max values of marker_temp)\n",
    "                series_comp = sort(series, dims=1,rev=true)[marker_temp_dc,:]\n",
    "                # Forming the centroids\n",
    "                k_cent_comp = copy(series)\n",
    "                (k_cent_comp[marker_temp,:],) = aggreg1D(series[marker_temp,:], rep_value)\n",
    "                # Ordering chunk with clustering instance in a decrescent order\n",
    "                k_cent_comp = sort(k_cent_comp, dims=1, rev=true)[marker_temp_dc,:]\n",
    "            else\n",
    "                # Part of series compared\n",
    "                series_comp = series[marker_temp,:]\n",
    "                # Centroids of the temporarily formed cluster (TODO: implement another method for aggreg1D receiving the clusters with respective weights)\n",
    "                (k_cent_comp,) = aggreg1D(series_comp, rep_value)\n",
    "            end\n",
    "            \n",
    "            # Distance computation\n",
    "            dist[k_search] = compute_dist(N, dm, series_comp, k_cent_comp)\n",
    "        end\n",
    "\n",
    "        # Find whenever the min_dist occurs first (i.e., using findmin()[2])\n",
    "        ## TODO: implement the multiple merges (e.g., using findall())\n",
    "        min_dist = findmin(dist)[2] |> Int        \n",
    "        merging_clust = min_dist:min_dist+block_size-1\n",
    "\n",
    "        # Create a flag to the positions in the series that will be aggregated\n",
    "        marker = update_marker(_SeriesInstance, _ClustInstance, min_dist)    \n",
    "\n",
    "        # Update _DistUpdate dictionary with the minimal distance found and the new marker\n",
    "        _DistUpdate = merge(+, _DistUpdate, Dict(marker => DistUpdate(min_dist,merging_clust)))\n",
    "\n",
    "        # Update clusters and series_clust\n",
    "        update_clust!(_ClustInstance, _SeriesInstance, min_dist)\n",
    "\n",
    "        # Update clustering values\n",
    "        series_clust = _ClustInstance.series_clust\n",
    "        weights = _ClustInstance.weights\n",
    "\n",
    "        nclusters = _ClustInstance.nclusters\n",
    "        k_cent = _ClustInstance.k_cent\n",
    "        search_range =_ClustInstance.search_range\n",
    "        \n",
    "        # Update number of clusters k\n",
    "        new_current_k = _ClustInstance.nclusters\n",
    "        update_k!(_SeriesInstance, new_current_k)\n",
    "        k = new_current_k\n",
    "\n",
    "        # Store series_clust and k_cent\n",
    "        _ClustUpdate = merge(+,_ClustUpdate,Dict(\"$k\" => copy(_ClustInstance)))\n",
    "        _SeriesUpdate = merge(+,_SeriesUpdate,Dict(\"$k\" => copy(_SeriesInstance)))\n",
    "\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "execute_inst (generic function with 1 method)"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function execute_inst(lseries::Int,nseries::Int,dc_mode::Bool; parallel=false)\n",
    "    \n",
    "    # Declare series\n",
    "    series = rand(lseries,nseries);\n",
    "    \n",
    "    # Forming SeriesInstance\n",
    "    block_size = 2\n",
    "    stopping_k = 1\n",
    "    current_k = lseries\n",
    "    rep_value = :mean\n",
    "    series_dc = sort(series, dims=1, rev=true)\n",
    "    ord_dc = reduce(hcat,sortperm.(collect(eachslice(series,dims=2)),rev=true))\n",
    "    \n",
    "    # Forming ClustInstance\n",
    "    k_cent = copy(series)\n",
    "    weights = ones(lseries) |> Vector{Int64}\n",
    "    series_clust = collect(1:lseries) |> Vector{Int64}\n",
    "    nclusters = lseries\n",
    "    search_range = 1:size(series,1)-block_size+1\n",
    "    dm = :ward\n",
    "\n",
    "    # Declaring instances\n",
    "    _SeriesInstance = load_series_instance(series,block_size,current_k,stopping_k,dm,rep_value,lseries,nseries,series_dc,ord_dc);\n",
    "    _ClustInstance = load_clust_instance(k_cent,series_clust,weights,search_range,dc_mode)\n",
    "\n",
    "    # Dictionary to keep the min distances and respective markers/min_dist found in each iteration\n",
    "    _DistUpdate = Dict{Vector{Bool}, DistUpdate}()\n",
    "\n",
    "    # Dictionaries to store series_clust and k_cent\n",
    "    _SeriesUpdate = Dict{String,SeriesInstance}()\n",
    "    _ClustUpdate = Dict{String,ClustInstance}();\n",
    "\n",
    "    if parallel\n",
    "        test_parallel(series,block_size,\n",
    "            stopping_k,\n",
    "            current_k,\n",
    "            dm,\n",
    "            rep_value,\n",
    "            lseries,\n",
    "            nseries,\n",
    "            series_dc,\n",
    "            ord_dc,\n",
    "            k_cent,\n",
    "            weights,\n",
    "            series_clust,\n",
    "            nclusters,\n",
    "            search_range,\n",
    "            dc_mode,\n",
    "            _SeriesInstance,\n",
    "            _ClustInstance,\n",
    "            _DistUpdate,\n",
    "            _SeriesUpdate,\n",
    "            _ClustUpdate)\n",
    "    else\n",
    "        test_serial(series,block_size,\n",
    "            stopping_k,\n",
    "            current_k,\n",
    "            dm,\n",
    "            rep_value,\n",
    "            lseries,\n",
    "            nseries,\n",
    "            series_dc,\n",
    "            ord_dc,\n",
    "            k_cent,\n",
    "            weights,\n",
    "            series_clust,\n",
    "            nclusters,\n",
    "            search_range,\n",
    "            dc_mode,\n",
    "            _SeriesInstance,\n",
    "            _ClustInstance,\n",
    "            _DistUpdate,\n",
    "            _SeriesUpdate,\n",
    "            _ClustUpdate)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: 100 lines random series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.531555 seconds (755.48 k allocations: 50.944 MiB, 1.96% gc time, 93.67% compilation time)\n"
     ]
    }
   ],
   "source": [
    "execute_inst(100,4,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.058310 seconds (214.85 k allocations: 22.232 MiB, 20.28% gc time)\n"
     ]
    }
   ],
   "source": [
    "execute_inst(100,4,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: 200 lines random series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_inst(200,4,true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_inst(200,4,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: 500 lines random series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_inst(500,4,true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_inst(500,4,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: parallelisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13.788115 seconds (5.39 M allocations: 8.609 GiB, 8.58% gc time)\n"
     ]
    }
   ],
   "source": [
    "execute_inst(500, 4, true, parallel=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5.665794 seconds (7.84 M allocations: 8.680 GiB, 29.55% gc time, 1.92% compilation time)\n"
     ]
    }
   ],
   "source": [
    "execute_inst(500, 4, true, parallel=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.0",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
